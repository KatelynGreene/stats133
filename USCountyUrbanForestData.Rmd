---
title: "USCountyUrbanForestData"
author: "Team Senioritis"
date: "4/21/17"
output: 
  html_document:
    fig_height: 3
    fig_width: 5 
---
<!-- Don't edit in between this line and the one below -->
```{r include=FALSE}
# Don't delete this chunk if you are using the DataComputing package
library(DataComputing) 
library(XML)
library(RCurl)
library(readxl)
```
*Source file* 
```{r, results='asis', echo=FALSE}
includeSourceDocuments()
```
<!-- Don't edit the material above this line -->

The purpose of this document is to extract urban forest from all the counties in the U.S.
The original data source is:
https://www.nrs.fs.fed.us/data/urban/ 

Extracts the links to each State from the main link:
https://www.nrs.fs.fed.us/data/urban/
```{r}
URL <- "https://www.nrs.fs.fed.us/data/urban/"
txt <- getURLContent(URL)
doc <- htmlParse(txt)

#Scrape the county name
stateNames <- xpathSApply(doc, '//ul/li/a/strong', xmlValue)

#****NOTE: Missing Alaska and Hawaii*******
stateLinks <- xpathSApply(doc, '//ul[@class="state_list"]/li/a/@href')
baseURL <- "https://www.nrs.fs.fed.us"
stateLinks <- paste(baseURL,as.character(stateLinks),sep="")

#Data Frame of StateName and stateLink
AllStates <- data.frame(stateNames, stateLinks, stringsAsFactors = FALSE)
head(AllStates) 
```

Extracts the excel file download link for each state
```{r}
downloadLinks <- vector(mode="character", length=length(AllStates$stateNames))

for (i in 1:length(stateLinks)){
  stateURL<- stateLinks[i]
  stateTxt <- getURLContent(stateURL)
  stateDoc <- htmlParse(stateTxt)
  downloadLink <- xpathSApply(stateDoc, '//ol[@id="data_options"]/li/a/@href')

  #The HTML source code is poor so need to use grepl to extract .xls from Xpath results
  length(downloadLink)
  for (j in 1:length(downloadLink)){
     if (grepl(".xls", downloadLink[j])){
      downloadLinks[i] <- downloadLink[j]
      break 
    }
  }
}

AllStates$downloadLinks <- downloadLinks

AllStates$downloadLinks
 
```


Download each excel file, load it into R as a dataframe, and assign the data frame to the respective state name.
NOTE THE DOWNLOAD TIME IS ENORMOUS!!! DON'T RUN THIS ON ALL STATES YET, IT CRASHES EVERYTHING.
```{r}
library(readxl)
for(i in 1:1){ #length(AllStates$stateNames)){
  destination <- paste("C:\\Users\\Katrlyn\\Downloads\\", AllStates$stateNames[i], ".xls", sep = "")
  #Works but the issue is that file must be handled locally during preprocessing.
  download.file(AllStates$downloadLinks[i], destfile = destination, mode = "wb" )

  test <- read_excel(destination, sheet = 10, skip = 3)
  
  assign(AllStates$stateNames[i], test)
  
}
```

Accessing the stored tables
```{r}
head(Alabama)

#Accessing names:
#as.name(countyName[1])

```



#'/Users/anishakumar/Desktop'
  

#this works '//div[@id="bio"]/p'
#'//div[@id="bio"]/p/ol[@id = "data_options"]/li/a'
```
for (j in 1:length(stateExcel)){
    if grepl( "xls$", stateExcel[j]){ 
      stateExcel <- stateExcel[j]
      break
    }
  }

