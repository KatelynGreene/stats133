---
title: "JoinAllTables"
author: "Vanessa"
date: ""
output: 
  html_document:
    fig_height: 3
    fig_width: 5
---
<!-- Don't edit in between this line and the one below -->
```{r include=FALSE}
# Don't delete this chunk if you are using the DataComputing package
library(DataComputing)
library(RCurl)
library(readxl)
library(mosaic)
library(readr)
library(tidyr) 
library(dplyr)
library(stringi)
library(XML)
library(rvest)
library(rgdal)
```
*Source file* 
```{r, results='asis', echo=FALSE}
includeSourceDocuments()
```
<!-- Don't edit the material above this line -->


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Downloading the USACountyEduAttainment data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```{r}
#' @Variables of interest: UrbanRank (Anisha is currently  making it) and Bachelors (%) 
#' @Year 2013 Rural-Urban Continuum Codes, 2011-2015 Education Attainment

#' Scrapes the excel download link for US County Education Attainment
#' Source: "https://www.ers.usda.gov/data-products/county-level-data-sets/county-level-data-sets-download-data.aspx" and click Educational attainment for the U.S., States, and counties, 1970-2015
#'
#' @return The edu_data that contains the FIPSCode, UrbanCode (2013 Rural-Urban Continuum Code), LessThanHS (%), HSDiploma (%), Bachelors (%), UrbanRank. 

GetEduData <- function(){
  
  #Use downoload link to read the data
  url <- "https://www.ers.usda.gov/webdocs/DataFiles/CountyLevel_Data_Sets_Download_Data__18026/Education.xls?v=42762"
  #tmp is a temporary file in which the data from the excel sheet is stored 
  tmp <- tempfile(fileext=".xls")
  download.file(url, destfile=tmp, mode="wb")
  #read data from sheet 1 beginning at row 5
  edu_data_unclean <- read_excel(tmp, sheet = 1, skip = 4)
  
  #Get rid of the temporary file tmp
  unlink(tmp)
  
  #Wrangle and select variables
  
  #Choose the columns from edu_data_clean that has FIPS Code, 2013 Rural-Urban Continuum Code and any columns that have information from 2011
  edu_data<-edu_data_unclean[, grep("2011|FIPS Code|2013 Rural", colnames(edu_data_unclean))] 
  
  #Choose columns from edu_data taht has FIPSCode, 2013 Rural-Urban Continuum Code and Percent
  edu_data<-edu_data[, grep("Percent|FIPS Code|2013 Rural", colnames(edu_data))]
  
  #Rename FIPS Code column to FIPSCode
  edu_data<-edu_data%>%dplyr::rename(FIPSCode=`FIPS Code`)
  
  #Rename the columns so that they are cleaner 
  colnames(edu_data)<-c('FIPSCode', 'UrbanCode', 'LessThanHS', 'HSDiploma', 'SomeCollege','Bachelors')
  
}


```

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Downloading the PollutionPerCounty data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```{r}


GetPollutionData <- function(){
  #dowload/wrangle the PollutionPerCounty data
  poll_county_unclean<-read.csv("https://data.cdc.gov/api/views/cjae-szjv/rows.csv?accessType=DOWNLOAD")
  poll_county<-poll_county_unclean %>%
    filter(MeasureType=='Average')%>%
    group_by(ReportYear)%>%
    arrange(CountyName)%>%
    arrange(StateName)%>%
    filter(MeasureId==296)%>%
    dplyr::rename(FIPSCode=CountyFips)%>%
    subset(select=c(FIPSCode, ReportYear, Value, Unit))%>%
    mutate(FIPSCode=gsub("46113","46102", FIPSCode))%>%
    filter(ReportYear==2010)%>%
    subset(select=-c(ReportYear))
  poll_county$FIPSCode<-stri_pad_left(poll_county$FIPSCode, 5, "0")  
  
  colnames(poll_county) <- c("FIPSCode", "Pollution", "Unit")
  
  return(poll_county)
}




```

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Downloading the CountyReferenceTable data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

```{r}

#download/clean CountyReferenceTable data

GetCountyReference <- function(){
  
  #Webscrape/clean CountyReferenceTable data
  every_us_county <- 
    "https://en.wikipedia.org/wiki/List_of_United_States_counties_and_county_equivalents" %>%
    read_html() %>%
    html_nodes(xpath = '//*[@id="mw-content-text"]/table') %>%
    html_table(fill=TRUE)
  every_us_county2<-every_us_county[[2]]
  every_county<-every_us_county2%>%
    dplyr::rename(FIPSCode=INCITS, County=`County or equivalent`, State=`State or district`)%>%
    subset(select=c( `FIPSCode`, `County`, `State`))
  every_county$FIPSCode<-stri_pad_left(every_county$FIPSCode, 5, "0")
  every_county<-mutate(every_county,StateFips=substr(every_county$FIPSCode,1,2))
  #View(every_county)
  
  return(every_county)

}



```

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Downloading the USACountyIncomeEmploy data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

```{r}
#' @Variables of interest: Income and IncomeQuartile
#' @Year 2015

#' Scrapes the excel download link for US Income
#' Source: https://www.ers.usda.gov/data-products/county-level-data-sets/county-level-data-sets-download-data.aspx and click Unemployment and median household income for the U.S., States, and counties, 2007-15
#'
#' @return The income dataframe that contains FIPSCode, Income, Unemployment and IncomeQuartile 

GetIncomeData <- function(){
  
  #download the data from the URL
  url <- "https://www.ers.usda.gov/webdocs/DataFiles/CountyLevel_Data_Sets_Download_Data__18026/Unemployment.xls?v=42762"
  #temporarily store the file
  tmp <- tempfile(fileext=".xls")
  download.file(url, destfile=tmp, mode="wb")
  
  income_unclean <- read_excel(tmp, sheet = 1, skip = 7)
  
  #remove the temporary file
  unlink(tmp)
  
  #remove any non-alphanumeric characters
  income <- income_unclean %>% 
    mutate(Area_name= gsub(", ..$", "", Area_name), Area_name)%>% #remove any non-alphanumeric characters
    subset(select=c("FIPStxt","Median_Household_Income_2015","Unemployment_rate_2015"))%>% #select for desired variables
    dplyr::rename(FIPSCode=FIPStxt) #Rename FIPStxt so that it is FIPSCode
  
  #Rename the column names 
  colnames(income) <- c("FIPSCode", "Income", "Unemployment")
  
  #Make a column called IncomeQuartile that separates incomes into 4 quartiles
  income$IncomeQuartile <- ntile(income$Income, 4) 
  
  #Name the income levels 
  income$IncomeQuartile[income$IncomeQuartile == 1] <- "< $40k" 
  income$IncomeQuartile[income$IncomeQuartile == 2] <- "$40k - 47k"
  income$IncomeQuartile[income$IncomeQuartile == 3] <- "$47k - 54k"
  income$IncomeQuartile[income$IncomeQuartile == 4] <- "> $54k"
  income$IncomeQuartile <- base::as.factor(income$IncomeQuartile)
  #income$IncomeQuartile = factor(income$IncomeQuartile, levels(income$IncomeQuartile) [c(3,1,2,4)]) #FOR PC
  income$IncomeQuartile <- factor(income$IncomeQuartile, levels = c("< $40k","$40k - 47k","$47k - 54k","> $54k")) #FOR MAC 
  return(income)
}


```

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Downloading the USCountyUrbanForest data 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


The purpose of this document is to extract urban forest from all the counties in the U.S.


#' Scrapes the excel download link for each state in the U.S.
#' Source: https://www.nrs.fs.fed.us/data/urban/
#'
#' @param x A number
#' @param y A number
#' @return The sum of \code{x} and \code{y}

```{r}
#' Scrapes the excel download link for each state in the U.S.
#'
#' @return A data frame containing variables: stateNames and downloadLinks (Excel file download links)

Webscrape <- function(){
  
  #----------------------------------------------
  #Scrape the link to each state from the US data
  #----------------------------------------------

  library(XML)
  library(RCurl)
  library(readxl)
  
  URL <- "https://www.nrs.fs.fed.us/data/urban/"
  txt <- getURLContent(URL)
  doc <- htmlParse(txt)
  
  #Scrape the state name
  stateNames <- xpathSApply(doc, '//ul/li/a/strong', xmlValue)
  
  #Scrape the state link
  stateLinks <- xpathSApply(doc, '//ul[@class="state_list"]/li/a/@href')
  baseURL <- "https://www.nrs.fs.fed.us"
  stateLinks <- paste(baseURL,as.character(stateLinks),sep="")
  
  #Data Frame of StateName and stateLink
  AllStates <- data.frame(stateNames, stateLinks, stringsAsFactors = FALSE)

  #Fix naming conventions standard Washington DC --> District of Columbia
  AllStates$stateNames[AllStates$stateNames == "Washington, D.C"] <- "District of Columbia"
  
  #-------------------------------------------------------
  #Scrape the state xls file download link from each state page
  #-------------------------------------------------------
  
  downloadLinks <- vector(mode="character", length=length(AllStates$stateNames))
  
  for (i in 1:length(stateLinks)){
    stateURL<- stateLinks[i]
    stateTxt <- getURLContent(stateURL)
    stateDoc <- htmlParse(stateTxt)
    downloadLink <- xpathSApply(stateDoc, '//ol[@id="data_options"]/li/a/@href')
  
    #The HTML source code is poor so need to use grepl to extract .xls from Xpath results
    length(downloadLink)
    for (j in 1:length(downloadLink)){
       if (grepl(".xls", downloadLink[j])){
        downloadLinks[i] <- downloadLink[j]
        break 
      }
    }
  }
  
  # Add to dataframe
  AllStates$downloadLinks <- downloadLinks
  
  # Return only the relevant part of the database
  AllStates <- AllStates %>% select(stateNames, downloadLinks)
  
  return(AllStates)
}

```

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Downloading and loading Excel files 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


All the support functions:

```{r}
#' Creates a reference dataframe that maps county name to FIPS code based on the 2010 census, with naming updates up to 2015.

#' @param state (optional) to restrict FIPS codes to one state
#' @return A data frame containing variables: State, CountyName, and FIPS (FIPS code)

FIPS_fun <- function(state = NA){
  #Note: colClasses = "character" helps keep the leading 0s
  #http://stackoverflow.com/questions/17414776/read-csv-warning-eof-within-quoted-string-prevents-complete-reading-of-file
  df <- read.table("https://www2.census.gov/geo/docs/reference/codes/files/national_county.txt", sep = ",", col.names = c("State", "StateFIPS", "CountyFIPS", "CountyName", "ClassFIPSCode" ), colClasses = "character", quote = "")
  
  #Shannon County (46-113) change to Oglala Lakota County (46-102) (Effective 2015)
  df2 <- data.frame(State = "SD", StateFIPS = "46", CountyFIPS = "102", CountyName = "Oglala Lakota County", ClassFIPSCode = "H1")
  df <- rbind(df, df2)
  
  #Merge state and county into one FIPS code
  FIPS_base <- df %>% mutate(FIPS = paste(StateFIPS, CountyFIPS, sep = ""))
  
  #Delete Shannon County (It changed to Oglala Lakota)
  FIPS_base <- FIPS_base %>% filter(FIPS != "46113")
  
  #return FIPS codes of the state passed in
  if(!is.na(state)){
    FIPS_base <- FIPS_base %>% subset(State == state) %>% select(State, CountyName, FIPS)
    return(FIPS_base)
  }else{return(FIPS_base)}
  
}

```


```{r}
#' Extracts data from an excel file relating to the:
#' Tree canopy (m2/person), Available green space (ha), and Tree canopy cover in developped regions (%) 
#' for each county in the state. This data, along with the county name and the FIPS code is returned as a data frame.

#' @param file Location of the state .xls file
#' @return A data frame containing variables: State, CountyName, FIPS (FIPS code), TreeCanopy, AvailGreenSpace, and TreeCanopyCover

ExtractStateData <- function(file, stateAbbrev){
  
  # ---------------------------------------------
  # Read-in relevant sheets from the excel files
  # ---------------------------------------------
  # Known bugs: NC 7th sheet is actually sheet 8.
  # Solution: sheet = string instead of sheet = integer when reading the excel file.
  # Known Bug: DC has fewer sheets.
  # Solution: various fixes changing DC to District of Columbia and reading sheet 5 & 8 instead. 
  
  #tmp = "C:\\Users\\kagex\\stats133Project\\AllStates\\Illinois.xls"
  #stateAbbrev = "IL"
  
  if(stateAbbrev == "DC"){
    xl_7 <- read_excel(file, sheet = "5", skip = 3)
    xl_10 <- read_excel(file, sheet = "8", skip = 4) 
  }else{
    xl_7 <- read_excel(file, sheet = "7", skip = 3)
    xl_10 <- read_excel(file, sheet = "10", skip = 3)
  }
  
  # -----------------------------------
  # Clean and select relevant variables
  # -----------------------------------
  
  xl_7 <- xl_7 %>% select(c(`X__1`, `m2/person__1`, `Available green space (ha)`))
  #Units: Tree canopy Covering (m2/person), Available green space (ha)
  colnames(xl_7) <- c("CountyName","TreeCanopy", "AvailGreenSpace")
  
  xl_10 <- xl_10 %>% select(c(`X__1`, `Tree % h`))
  #Tree canopy cover in developped regions (%)
  colnames(xl_10) <- c("CountyName", "TreeCanopyCover")
  
  #Exclude the variable descriptions at the end of the sheet
  xl_10 <- na.omit(xl_10)
  
  #Join the two excel sheets to create one datframe of county data
  joined <- full_join(xl_7, xl_10, by = "CountyName")
  #get ride of statewide summary row
  joined_clean <- joined %>% subset(CountyName != "Statewide")
  
  #Add column of state Abbreviations to assist debugging later
  #joined_clean$StateAbb <- rep(stateAbbrev, times = length(joined_clean$CountyName))
  
  # ------------------
  # Naming Corrections:
  # -----------------

  # 1) Washington DC must be called District of Columbia to find FIPS code
  # 2) La Salle county in IL changed to LaSalle County in 2001 
  # 3) Clifton Forge city is no longer a county as of 2001
  # 4) Shannon County, SD changed to Oglala Dakota in 2015
  if(stateAbbrev == "DC"){
    #joined_clean[1, "CountyName"] <- "District of Columbia"
    joined_clean$CountyName[joined_clean$CountyName == "Washington, D.C."] <- "District of Columbia"
  }else if(stateAbbrev == "IL"){
    #joined_clean[49,"CountyName"] <- "LaSalle County"
    joined_clean$CountyName[joined_clean$CountyName == "La Salle County"] <- "LaSalle County"
  }else if(stateAbbrev == "VA"){
    joined_clean <- joined_clean %>% filter(CountyName != "Clifton Forge city")
  }else if(stateAbbrev == "SD"){
    joined_clean$CountyName[joined_clean$CountyName == "Shannon County"] <- "Oglala Lakota County"
  }
  
  # Add FIPS codes to data
  FIPS_base <- FIPS_fun(state = stateAbbrev)
  final_df <- full_join(joined_clean, FIPS_base, by = "CountyName")
  
  return(final_df)
}

```

To keep from wasting time redownloading files, use this code to download once and use your local files. 
```{r, eval=FALSE}
#' Downloads excel files to a local directory for later extraction and cleaning. 
#' 
#' @param AllStates data frame that contains stateNames and downloadLinks
#' @return Data frame with stateNames, downloadLinks, and fileLocations 

DownloadLocally <- function(AllStates){
  
  library(readxl)
  
  #Stores locations for future file reading
  fileLocations <- vector(mode="character", length=length(AllStates$stateNames))
  
  #Download file for each state
  for(i in 1:length(AllStates$stateNames)){
    destination <- paste("~/Downloads/stats133data/AllStates/", AllStates$stateNames[i], ".xls", sep = "")
    
    download.file(AllStates$downloadLinks[i], destfile = destination, mode = "wb" )
    
    fileLocations[i] <- destination
  }
  
  AllStates$fileLocations <- fileLocations
  return(AllStates)
  
}

```

Load the filepaths if you're using the downloaded files
```{r}
#' Loads the locations of all the excel files stored in a local directory for later extraction and cleaning. 
#' 
#' @param AllStates data frame that contains stateNames and downloadLinks
#' @return Data frame with stateNames, downloadLinks, and fileLocations 

LoadFilepaths <- function(AllStates, userName){
  if(userName == "K"){
    AllStates$fileLocations <- paste("C:\\Users\\kanay\\Documents\\R\\stat133\\AllStates\\", AllStates$stateNames, ".xls", sep = "")
  }else if(userName == "V"){
    AllStates$fileLocations <- paste("~/Downloads/stats133data/AllStates/", AllStates$stateNames, ".xls", sep = "")
  }else if(userName == "A") {
    AllStates$fileLocations <- paste("/Users/anishakumar/Documents/Stats133/RStudioFiles/Stats133Project/AllStates/", AllStates$stateNames, ".xls", sep = "")
  }else if(userName == "KS"){
    AllStates$fileLocations <- paste("C:\\Users\\kagex\\stats133Project\\AllStates\\", AllStates$stateNames, ".xls", sep = "")
  }else{
    AllStates$fileLocations <- paste("C:\\Users\\Katrlyn\\stats133Project\\AllStates\\", AllStates$stateNames, ".xls", sep = "")
  }
  

  return(AllStates)
}

```

------------------------------------------------------
Main code (Method 1: Download all the files locally)
------------------------------------------------------
```{r}

GetTreeData <- function(userName){
  #Webscrape the data links from the USDA Forest Service
  #The original data source is: https://www.nrs.fs.fed.us/data/urban/ 
  
  AllStates <- Webscrape()
  
  #Download all the xls files locally for data processing
  #AllStates <- DownloadLocally(AllStates)
  AllStates <- LoadFilepaths(AllStates, userName) #You will need to load this every time if you're working from local files
  
  #Extract urban forestry for each state in the United States
  for(i in 1:length(AllStates$downloadLinks)){
    print(AllStates$stateNames[i])
    #Get the current state postal abbreviation 
    if(AllStates$stateNames[i] == "District of Columbia"){
      stateAbbrev <- "DC"
    }else{
      stateAbbrev <- state.abb[match(AllStates$stateNames[i],state.name)]
    }
    
    #Extract the variables Tree canopy Covering (m2/person), 
    #Available green space (ha), and Tree canopy cover in developped regions (%) for the current state
    state_df <- ExtractStateData(AllStates$fileLocations[i], stateAbbrev)
    
    #Build a data frame of all the states
    if(i == 1){
      df_base <- state_df
    }else if( i == 2){
      df_full <- rbind(df_base, state_df)
    }else{
      df_full <- rbind(df_full, state_df)
    }
    #unlink(tmp)
    
  }
  
  View(df_full)
  
  #Things that need fixing still
  df <- df_full %>% subset(is.na(FIPS) | is.na(TreeCanopy))
  df
  
  #-------------------------------------------------------------------------------------------
  ###### MY CODE  SO THAT I CAN JOIN THE TREE DATA TO MY OTHER DATA FOR THE SCATTERPLOTS######
  #-------------------------------------------------------------------------------------------
  tree_data<-df_full[, -grep("State|Name", colnames(df_full))]
  tree_data<-tree_data%>%rename(FIPSCode=FIPS)%>%filter(TreeCanopyCover<.55)
  
  return(tree_data)
}


```


------------------------------------------------------
Main code (Method 2: Download via temporary files)
------------------------------------------------------


```{r, eval=FALSE}
#Webscrape the data links from the USDA Forest Service
#The original data source is: https://www.nrs.fs.fed.us/data/urban/ 

AllStates <- Webscrape()

#Download via temp files (no local storage on hard drive )

for(i in 1:length(AllStates$downloadLinks)){
  #Download xls file
  url <- AllStates$downloadLinks[i]
  tmp <- tempfile(fileext=".xls")
  download.file(url,destfile=tmp, mode="wb")
  
  #Extract urban forestry for each state in the United States
  print(AllStates$stateNames[i])
  #Get the current state postal abbreviation 
  if(AllStates$stateNames[i] == "District of Columbia"){
    stateAbbrev <- "DC"
  }else{
    stateAbbrev <- state.abb[match(AllStates$stateNames[i],state.name)]
  }
  
  #Extract the variables Tree canopy Covering (m2/person), Available green space (ha), and Tree canopy cover in developped regions (%) for the current state
  state_df <- ExtractStateData(tmp, stateAbbrev)
  
  #Build a data frame of all the states
  if(i == 1){
    df_base <- state_df
  }else if( i == 2){
    df_full <- rbind(df_base, state_df)
  }else{
    df_full <- rbind(df_full, state_df)
  }
  unlink(tmp)

}

View(df_full)

#Things that need fixing still
df <- df_full %>% subset(is.na(FIPS) | is.na(TreeCanopy))
df
```



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Joining all datatables 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

```{r}

#-------USER--------
# READ BELOW BEFORE RUNNING
#--------------------
#Put the first initial of your first name.
userName = "A"


all_data=NULL

every_county <- GetCountyReference()

edu_data <- GetEduData()
income_data <- GetIncomeData()
poll_data <- GetPollutionData()
tree_data <- GetTreeData(userName)


all_data<-plyr::join_all(list(every_county, edu_data, income_data, poll_data, tree_data), by='FIPSCode', type='full')
all_data<-na.omit(all_data)


head(all_data)

#save(all_data, file="all_data.RData")

```



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Scatterplots
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```{r eval = FALSE}
# use ggplot to make a scatter plot of education versus TREE DATA

#all_data_narrow_tree<-all_data_narrow%>%gather(key=TreeType, value=TreeValue, TreeCanopy,AvailGreenSpace, TreeCanopyCover)
edu_treecanop_plot<-all_data%>%
  ggplot(aes(x=TreeCanopyCover, y=coll_HS))+
  geom_point(aes(color=IncomeQuartile))+
  facet_grid(.~IncomeQuartile)

edu_treecanop_plot

```

```{r eval = FALSE}
#use ggplot to plot edication data versus income data
#edu_inc_plot<-all_dataw%>%
  #ggplot(aes(x=PercentOfAdults, y=Income))+
  #geom_point(aes(color=EducationLevel))+
  #facet_grid(EducationLevel~.)
#View(all_data_narrow)
#edu_inc_plot
```

```{r eval = FALSE}

#use ggplot to graph education versus pollution
edu_poll_graph<-all_data%>%
  filter(StateFips!=02)%>%
  ggplot(aes(x=Pollution, y=coll_HS))+
  geom_point(aes(color=IncomeQuartile))+
  facet_grid(.~IncomeQuartile)

edu_poll_graph  
```

```{r eval = FALSE}
#Linear Regerssion Time......
library(statisticalModeling)
library(rpart)
#install.packages("rpart.plot")
library(rpart.plot)
library(stats)
library(mosaicData)

#Tree reg
et1<-lm(coll_HS~TreeCanopyCover, data=all_data)
et2<-lm(coll_HS~TreeCanopyCover +Income, data=all_data)
stargazer::stargazer(et1, et2, type="text",
 dep.var.labels=c("Ratio of People with Higher Education to Lower Education"),
 covariate.labels=c("Tree Canopy Cover","Median Income"), out="tree_edu.txt")

#pollution regressions


ep1<-lm(coll_HS~Pollution, data=all_data) 
ep2<-lm(coll_HS~Pollution+Income, data=all_data) 

stargazer::stargazer(ep1, ep2, type="text",
 dep.var.labels=c("Ratio of People with Higher Education to Lower Education"),
 covariate.labels=c("Particulate Matter","Median Income"), out="polution_edu.txt")
```


