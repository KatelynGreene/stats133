---
title: "JoinAllTables"
author: "Vanessa"
date: ""
output: 
  html_document:
    fig_height: 3
    fig_width: 5
---
<!-- Don't edit in between this line and the one below -->
```{r include=FALSE}
# Don't delete this chunk if you are using the DataComputing package
library(DataComputing)
library(RCurl)
library(readxl)
library(mosaic)
library(readr)
library(tidyr) 
library(dplyr)
library(stringi)
library(XML)
library(rvest)
library(rgdal)
```
*Source file* 
```{r, results='asis', echo=FALSE}
includeSourceDocuments()
```
<!-- Don't edit the material above this line -->


#' Scrapes the excel download link for each state in the U.S.
#' Source: https://www.nrs.fs.fed.us/data/urban/
#'
#' @param x A number
#' @param y A number
#' @return The sum of \code{x} and \code{y}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Downloading the USACountyEduAttainment data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```{r}
#' Scrapes the excel download link for US County Education Attainment
#' Source: "https://www.ers.usda.gov/data-products/county-level-data-sets/county-level-data-sets-download-data.aspx" and click Educational attainment for the U.S., States, and counties, 1970-2015
#' @Variables of interest: Rural-Urban Continuum code and Bachelors (%) 
#' @Year 2013 Rural-Urban Continuum Codes, 2011-2015 Education Attainment
#'
#' @return The edu_data that contains the FIPSCode, UrbanCode (2013 Rural-Urban Continuum Code), LessThanHS (%), HSDiploma (%), Bachelors (%), UrbanRank. 

GetEduData <- function(){
  
  # #Use downoload link to read the data
  url <- "https://www.ers.usda.gov/webdocs/DataFiles/48747/Education.xls?v=42762"
  #tmp is a temporary file in which the data from the excel sheet is stored
  tmp <- tempfile(fileext=".xls")
  download.file(url, destfile=tmp, mode="wb")
  #read data from sheet 1 beginning at row 5
  edu_data_unclean <- read_excel(tmp, sheet = 1, skip = 4)

  #Get rid of the temporary file tmp
  unlink(tmp)
  
  #Wrangle and select variables
  
  #Choose the columns from edu_data_clean that has FIPS Code, 2013 Rural-Urban Continuum Code and any columns that have information from 2011
  edu_data<-edu_data_unclean[, grep("2011|FIPS Code|2013 Rural", colnames(edu_data_unclean))] 
  
  #Choose columns from edu_data taht has FIPSCode, 2013 Rural-Urban Continuum Code and Percent
  edu_data<-edu_data[, grep("Percent|FIPS Code|2013 Rural", colnames(edu_data))]
  
  #Rename FIPS Code column to FIPSCode
  edu_data<-edu_data%>%dplyr::rename(FIPSCode=`FIPS Code`)
  
  #Rename the columns so that they are cleaner 
  colnames(edu_data)<-c('FIPSCode', 'UrbanCode', 'LessThanHS', 'HSDiploma', 'SomeCollege','Bachelors')
  
  edu_data$UrbanRank <- ""
  
  #The for loop classifies the UrbanCode (Rural-Urban Continuum Code). 
  #Code 1-3 is metropolitan, 4-7 is urban and 8 - 11 is rural.
  for (i in 1:nrow(edu_data)){
     if (is.na(edu_data$UrbanCode[i])){
       edu_data$UrbanRank[i] <- NA
      } else if (edu_data$UrbanCode[i] <= 3){
        edu_data$UrbanRank[i] <- "Metropolitan"
      } else if (edu_data$UrbanCode[i] <= 7){
        edu_data$UrbanRank[i] <- "Urban"
      } else {
        edu_data$UrbanRank[i] <- "Rural"
      }
  }
  edu_data$UrbanRank <- factor(edu_data$UrbanRank, levels = c("Rural", "Urban", "Metropolitan"))
  return(edu_data)
}


```

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Downloading the Pollution per County data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

```{r}
#' Downloads and wrangles pollution data in the form of air quality measurements from the EPA. 
#' Source: https://data.cdc.gov/dataset/Air-Quality-Measures-on-the-National-Environmental/cjae-szjv
#' @Variables: Annual average ambient concentrations of PM2.5 in micrograms per cubic meter 
#' (based on seasonal averages and daily measurement)
#' @Year: 2010
#'
#' @return data frame containing air quality for each county in the united states

GetPollutionData <- function(){
  poll_county_unclean<-read.csv("https://data.cdc.gov/api/views/cjae-szjv/rows.csv?accessType=DOWNLOAD")
  poll_county<-poll_county_unclean %>%
    filter(MeasureType=='Average')%>%
    group_by(ReportYear)%>%
    arrange(CountyName)%>%
    arrange(StateName)%>%
    filter(MeasureId==296)%>%
    dplyr::rename(FIPSCode=CountyFips)%>%
    subset(select=c(FIPSCode, ReportYear, Value))%>%
    #Fix Shannon County change to Oglala County SD
    mutate(FIPSCode=gsub("46113","46102", FIPSCode))%>%
    filter(ReportYear==2010)%>%
    subset(select=-c(ReportYear))
  poll_county$FIPSCode<-stri_pad_left(poll_county$FIPSCode, 5, "0")  
  colnames(poll_county) <- c("FIPSCode", "Pollution")
  return(poll_county)
}
```

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Downloading the County Reference Table data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

```{r}
#' Webscrapes and wrangles a table of every US county and its corresponding FIPS code and state name. 
#' Source: https://en.wikipedia.org/wiki/List_of_United_States_counties_and_county_equivalents
#' @Variables: FIPS code, County, State
#' @Year: 2014
#'
#' @return data frame containing every county name and its 
#' corresponding county FIPS code, state FIPS code, and state name.

GetCountyReference <- function(){
  every_us_county <- 
    "https://en.wikipedia.org/wiki/List_of_United_States_counties_and_county_equivalents" %>%
    read_html() %>%
    html_nodes(xpath = '//*[@id="mw-content-text"]/table') %>%
    html_table(fill=TRUE)
  every_us_county2<-every_us_county[[2]]
  every_county<-every_us_county2%>%
    dplyr::rename(FIPSCode=INCITS, County=`County or equivalent`, State=`State or district`)%>%
    subset(select=c( `FIPSCode`, `County`, `State`))
  every_county$FIPSCode<-stri_pad_left(every_county$FIPSCode, 5, "0")
  every_county<-mutate(every_county,StateFips=substr(every_county$FIPSCode,1,2))
  return(every_county)
}
```

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Downloading the USACountyIncomeEmploy data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

```{r}
#' Scrapes the excel download link for US Income
#' Source: https://www.ers.usda.gov/data-products/county-level-data-sets/county-level-data-sets-download-data.aspx #' (Unemployment and median household income for the U.S., States, and counties, 2007-15)
#' @Variables of interest: Income and IncomeQuartile
#' @Year 2015
#'
#' @return The income dataframe that contains FIPSCode, Income, Unemployment and IncomeQuartile 

GetIncomeData <- function(){
  url <- "https://www.ers.usda.gov/webdocs/DataFiles/48747/Unemployment.xls?v=42762"
  tmp <- tempfile(fileext=".xls")
  download.file(url, destfile=tmp, mode="wb")

  income_unclean <- read_excel(tmp, sheet = 1, skip = 7)
  unlink(tmp)
  
  #remove any non-alphanumeric characters
  income <- income_unclean %>% 
    mutate(Area_name= gsub(", ..$", "", Area_name), Area_name)%>% #remove any non-alphanumeric characters
    subset(select=c("FIPStxt","Median_Household_Income_2015","Unemployment_rate_2015"))%>% 
    dplyr::rename(FIPSCode=FIPStxt)
  colnames(income) <- c("FIPSCode", "Income", "Unemployment")
  
  #Make a column called IncomeQuartile that separates incomes into 4 quartiles
  income$IncomeQuartile <- ntile(income$Income, 4) 
  
  #Name the income levels 
  income$IncomeQuartile[income$IncomeQuartile == 1] <- "< $40k" 
  income$IncomeQuartile[income$IncomeQuartile == 2] <- "$40k - 47k"
  income$IncomeQuartile[income$IncomeQuartile == 3] <- "$47k - 54k"
  income$IncomeQuartile[income$IncomeQuartile == 4] <- "> $54k"
  income$IncomeQuartile <- base::as.factor(income$IncomeQuartile)
  income$IncomeQuartile <- factor(income$IncomeQuartile, levels = c("< $40k","$40k - 47k","$47k - 54k","> $54k")) 
  return(income)
}


```


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Downloading the USCountyUrbanForest data 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



The purpose of this document is to extract urban forest from all the counties in the U.S.

```{r}
#' Scrapes the excel download link for each state in the U.S.
#'
#' @return A data frame containing variables: stateNames and downloadLinks (Excel file download links)

Webscrape <- function(){
  #----------------------------------------------
  #Scrape the link to each state from the US data
  #----------------------------------------------
  library(XML)
  library(RCurl)
  library(readxl)
  URL <- "https://www.nrs.fs.fed.us/data/urban/"
  txt <- getURLContent(URL)
  doc <- htmlParse(txt)
  #Scrape the state name
  stateNames <- xpathSApply(doc, '//ul/li/a/strong', xmlValue)
  #Scrape the state link
  stateLinks <- xpathSApply(doc, '//ul[@class="state_list"]/li/a/@href')
  baseURL <- "https://www.nrs.fs.fed.us"
  stateLinks <- paste(baseURL,as.character(stateLinks),sep="")
  #Data Frame of StateName and stateLink
  AllStates <- data.frame(stateNames, stateLinks, stringsAsFactors = FALSE)
  #Fix naming conventions standard Washington DC --> District of Columbia
  AllStates$stateNames[AllStates$stateNames == "Washington, D.C"] <- "District of Columbia"
  #-------------------------------------------------------
  #Scrape the state xls file download link from each state page
  #-------------------------------------------------------
  downloadLinks <- vector(mode="character", length=length(AllStates$stateNames))
  for (i in 1:length(stateLinks)){
    stateURL<- stateLinks[i]
    stateTxt <- getURLContent(stateURL)
    stateDoc <- htmlParse(stateTxt)
    downloadLink <- xpathSApply(stateDoc, '//ol[@id="data_options"]/li/a/@href')
    #The HTML source code is poor so need to use grepl to extract .xls from Xpath results
    length(downloadLink)
    for (j in 1:length(downloadLink)){
       if (grepl(".xls", downloadLink[j])){
        downloadLinks[i] <- downloadLink[j]
        break 
      }
    }
  }
  # Add to dataframe
  AllStates$downloadLinks <- downloadLinks
  # Return only the relevant part of the database
  AllStates <- AllStates %>% select(stateNames, downloadLinks)
  return(AllStates)
}

```

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Support functions for Downloading and loading state Excel files 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

```{r}
#' Creates a reference dataframe that maps county name to FIPS code based on the 2010 census, 
#' with naming updates up to 2015.
#'
#' @param state (optional) to restrict FIPS codes to one state
#' @return A data frame containing variables: State, CountyName, and FIPS (FIPS code)

FIPS_fun <- function(state = NA){

  df <- read.table("https://www2.census.gov/geo/docs/reference/codes/files/national_county.txt", sep = ",", col.names = c("State", "StateFIPS", "CountyFIPS", "CountyName", "ClassFIPSCode" ), colClasses = "character", quote = "")
  #Shannon County (46-113) change to Oglala Lakota County (46-102) (Effective 2015)
  df2 <- data.frame(State = "SD", StateFIPS = "46", CountyFIPS = "102", CountyName = "Oglala Lakota County", ClassFIPSCode = "H1")
  df <- rbind(df, df2)
  #Merge state and county into one FIPS code
  FIPS_base <- df %>% mutate(FIPS = paste(StateFIPS, CountyFIPS, sep = ""))
  #Delete Shannon County (It changed to Oglala Lakota)
  FIPS_base <- FIPS_base %>% filter(FIPS != "46113")
  #return FIPS codes of the state passed in
  if(!is.na(state)){
    FIPS_base <- FIPS_base %>% subset(State == state) %>% select(State, CountyName, FIPS)
    return(FIPS_base)
  }else{return(FIPS_base)}
}
```

```{r}
#' Extracts data from an excel file relating to the forestry variables of interest.
#' @Variables: Tree canopy (m2/person), Available green space (ha), and Tree canopy cover in developped regions (%) 
#' @param file Location of the state .xls file
#' @param stateAbbrev the state postal code for the count of interest
#' @return A data frame containing variables: State, CountyName, FIPS (FIPS code), TreeCanopy, AvailGreenSpace, and TreeCanopyCover

ExtractStateData <- function(file, stateAbbrev){
  # ---------------------------------------------
  # Read-in relevant sheets from the excel files
  # ---------------------------------------------
  if(stateAbbrev == "DC"){
    xl_7 <- read_excel(file, sheet = "5", skip = 3)
    xl_10 <- read_excel(file, sheet = "8", skip = 4) 
  }else{
    xl_7 <- read_excel(file, sheet = "7", skip = 3)
    xl_10 <- read_excel(file, sheet = "10", skip = 3)
  }
  # -----------------------------------
  # Clean and select relevant variables
  # -----------------------------------
  xl_7 <- xl_7 %>% select(c(`X__1`, `m2/person__1`, `Available green space (ha)`))
  #Units: Tree canopy Covering (m2/person), Available green space (ha)
  colnames(xl_7) <- c("CountyName","TreeCanopy", "AvailGreenSpace")
  xl_10 <- xl_10 %>% select(c(`X__1`, `Tree % h`))
  #Tree canopy cover in developped regions (%)
  colnames(xl_10) <- c("CountyName", "TreeCanopyCover")
  #Exclude the variable descriptions at the end of the sheet
  xl_10 <- na.omit(xl_10)
  # --------
  # Join
  # --------
  #Join the two excel sheets to create one datframe of county data
  joined <- full_join(xl_7, xl_10, by = "CountyName")
  #get ride of statewide summary row
  joined_clean <- joined %>% subset(CountyName != "Statewide")
  # ------------------
  # Naming Corrections:
  # -----------------
  # 1) Washington DC must be called District of Columbia to find FIPS code
  # 2) La Salle county in IL changed to LaSalle County in 2001 
  # 3) Clifton Forge city is no longer a county as of 2001
  # 4) Shannon County, SD changed to Oglala Dakota in 2015
  if(stateAbbrev == "DC"){
    #joined_clean[1, "CountyName"] <- "District of Columbia"
    joined_clean$CountyName[joined_clean$CountyName == "Washington, D.C."] <- "District of Columbia"
  }else if(stateAbbrev == "IL"){
    #joined_clean[49,"CountyName"] <- "LaSalle County"
    joined_clean$CountyName[joined_clean$CountyName == "La Salle County"] <- "LaSalle County"
  }else if(stateAbbrev == "VA"){
    joined_clean <- joined_clean %>% filter(CountyName != "Clifton Forge city")
  }else if(stateAbbrev == "SD"){
    joined_clean$CountyName[joined_clean$CountyName == "Shannon County"] <- "Oglala Lakota County"
  }
  # Add FIPS codes to data
  FIPS_base <- FIPS_fun(state = stateAbbrev)
  final_df <- full_join(joined_clean, FIPS_base, by = "CountyName")
  return(final_df)
}

```

To keep from wasting time redownloading files, use this code to download once and use your local files. 
```{r, eval=FALSE}
#' Downloads excel files to a local directory for later extraction and cleaning. Note that the user will need to 
#' manually designate the desired download directory.
#' 
#' @param AllStates data frame that contains stateNames and downloadLinks
#' @return Data frame with stateNames, downloadLinks, and fileLocations 

DownloadLocally <- function(AllStates){
  library(readxl)
  #Stores locations for future file reading
  fileLocations <- vector(mode="character", length=length(AllStates$stateNames))
  #Download file for each state
  for(i in 1:length(AllStates$stateNames)){
    destination <- paste("~/Downloads/stats133data/AllStates/", AllStates$stateNames[i], ".xls", sep = "")
    download.file(AllStates$downloadLinks[i], destfile = destination, mode = "wb" )
    fileLocations[i] <- destination
  }
  AllStates$fileLocations <- fileLocations
  return(AllStates)
}

```

Load the filepaths if you're using the downloaded files
```{r}
#' Loads the locations of all the excel files stored in a local directory for later extraction and cleaning. 
#' Note that the user will need to designate the directory of their downloaded file.
#' 
#' @param AllStates data frame that contains stateNames and downloadLinks
#' @return Data frame with stateNames, downloadLinks, and fileLocations 

LoadFilepaths <- function(AllStates, userName){
  if(userName == "K"){
    AllStates$fileLocations <- paste("C:\\Users\\kanay\\Documents\\R\\stat133\\AllStates\\", AllStates$stateNames, ".xls", sep = "")
  }else if(userName == "V"){
    AllStates$fileLocations <- paste("~/Downloads/stats133data/AllStates/", AllStates$stateNames, ".xls", sep = "")
  }else if(userName == "A") {
    AllStates$fileLocations <- paste("/Users/anishakumar/Documents/Stats133/RStudioFiles/Stats133Project/AllStates/", AllStates$stateNames, ".xls", sep = "")
  }else if(userName == "KS"){
    AllStates$fileLocations <- paste("C:\\Users\\kagex\\stats133Project\\AllStates\\", AllStates$stateNames, ".xls", sep = "")
  }else{
    AllStates$fileLocations <- paste("C:\\Users\\Katrlyn\\stats133Project\\AllStates\\", AllStates$stateNames, ".xls", sep = "")
  }
  return(AllStates)
}

```

------------------------------------------------------
Download Tree Data  (Method 1: Download all the files locally)
------------------------------------------------------
```{r}
#' Scrapes the download link for each state's forestry data and then extracts forestry data for each county.
#' Source: https://www.nrs.fs.fed.us/data/urban/
#' @Variables: Tree canopy (m2/person), Available green space (ha), and Tree canopy cover in developped regions (%) 
#'
#' @return A data frame containing county name, FIPS code, tree canopy per person, available green space, and 
#' tree canopy cover in developped areas.

GetTreeData <- function(userName){
  AllStates <- Webscrape()
  
  #Download all the xls files locally for data processing
  #AllStates <- DownloadLocally(AllStates)
  AllStates <- LoadFilepaths(AllStates, userName) 
  
  #Extract urban forestry for each state in the United States
  for(i in 1:length(AllStates$downloadLinks)){
    print(AllStates$stateNames[i])
    #Get the current state postal abbreviation 
    if(AllStates$stateNames[i] == "District of Columbia"){
      stateAbbrev <- "DC"
    }else{
      stateAbbrev <- state.abb[match(AllStates$stateNames[i],state.name)]
    }
    
    #Extract the variables Tree canopy Covering (m2/person), 
    #Available green space (ha), and Tree canopy cover in developped regions (%) for the current state
    state_df <- ExtractStateData(AllStates$fileLocations[i], stateAbbrev)
    
    #Build a data frame of all the states
    if(i == 1){
      df_base <- state_df
    }else if( i == 2){
      df_full <- rbind(df_base, state_df)
    }else{
      df_full <- rbind(df_full, state_df)
    }
  }
  
  #Things that need fixing still
  df <- df_full %>% subset(is.na(FIPS) | is.na(TreeCanopy))
  df
  
  #Drop everything except for the variables of interest and FIPS code
  tree_data<-df_full[, -grep("State|Name", colnames(df_full))]
  #name to faciliate later join
  tree_data<-tree_data%>%rename(FIPSCode=FIPS)
  return(tree_data)
}


```


------------------------------------------------------
Download Tree Data (Method 2: Download via temporary files)
------------------------------------------------------


```{r, eval=FALSE}
#' Scrapes the download link for each state's forestry data and then extracts forestry data for each county.
#' Source: https://www.nrs.fs.fed.us/data/urban/
#' @Variables: Tree canopy (m2/person), Available green space (ha), and Tree canopy cover in developped regions (%) 
#'
#' @return A data frame containing county name, FIPS code, tree canopy per person, available green space, and 
#' tree canopy cover in developped areas.

GetTreeDataTEMP <- function(){

  AllStates <- Webscrape()
  
  #Download via temp files (no local storage on hard drive )
  
  for(i in 1:length(AllStates$downloadLinks)){
    url <- AllStates$downloadLinks[i]
    tmp <- tempfile(fileext=".xls")
    download.file(url,destfile=tmp, mode="wb")
    
    #Extract urban forestry for each state in the United States
    print(AllStates$stateNames[i])
    #Get the current state postal abbreviation 
    if(AllStates$stateNames[i] == "District of Columbia"){
      stateAbbrev <- "DC"
    }else{
      stateAbbrev <- state.abb[match(AllStates$stateNames[i],state.name)]
    }
    
    #Extract the variables Tree canopy Covering (m2/person), Available green space (ha), and Tree canopy cover in developped regions (%) for the current state
    state_df <- ExtractStateData(tmp, stateAbbrev)
    
    #Build a data frame of all the states
    if(i == 1){
      df_base <- state_df
    }else if( i == 2){
      df_full <- rbind(df_base, state_df)
    }else{
      df_full <- rbind(df_full, state_df)
    }
    unlink(tmp)
  }

  
  #Drop everything except for the variables of interest and FIPS code
  tree_data<-df_full[, -grep("State|Name", colnames(df_full))]
  #name to faciliate later join
  tree_data<-tree_data%>%rename(FIPSCode=FIPS)
  return(tree_data)
}

```


```{r}
#' Categorizes the counties into four regions (West, Midwest, South, and Northeast) 
#' by their geopgraphic region in the U.S., based on state FIPS.
#'
#' @param all_data Aggregate dataset
#'
#' @return revised aggregate data frame with county geographic region indicated

GetStateRegion4 <- function(all_data){
  all_data$Region <- ""
  for (i in 1:nrow(all_data)){
    if (all_data$StateFips[i] %in% c("53", "16", "32", "06", "41", "30", "56", "49", "08", "04", "35")){ 
      all_data$Region[i]<- "West"
    } else if (all_data$StateFips[i] %in% c("38", "46", "31", "20", "29", "19", "27", "55", "17", "18", "39", "26")){ 
      all_data$Region[i]<- "MidWest"
    } else if (all_data$StateFips[i] %in% c("40", "48", "22", "05", "28", "01", "12", "13", "47", "45", "37", "21", "51", "54", "24", "10", "11")){ 
      all_data$Region[i] <- "South"
    } else if (all_data$StateFips[i] %in% c("42", "34", "36", "09", "44", "25", "50", "33", "23")){
      all_data$Region[i] <- "NorthEast"
    } else 
      all_data$Region[i] <- NA
  }
  all_data$Region <- factor(all_data$Region, levels = c("West", "MidWest", "South", "NorthEast"))
  return(all_data)
}

```

---------
Arsenic 
---------

```{r}
#' Downloads Arsenic water quality data from USGS 
#' Source: https://water.usgs.gov/nawqa/trace/data/index.html#ARSENIC_NOV01
#' @Year:  1973 - 2001
#' @Variables: State name,FIPS code, arsenic conentraions, lat and long
#'
#' @return data frame containing variables of interest

downloadArsenic <- function(){
  url <- "https://water.usgs.gov/nawqa/trace/data/arsenic_nov2001.xls"
  tmp <- tempfile(fileext=".xls")
  download.file(url,destfile=tmp, mode="wb")
  arsenic <- read_excel(tmp, skip=58)
  
  # ----DATA CLEANING----
  # get rid of non-continental US data (Alaska, Puerto Rico, Virgin Islands)
  arsenic <- arsenic[!arsenic$STATE %in% c("AK","PR","VI"),]
  # only keep columns we need (especially becuase a few of the unnecessary columns loaded in really weird)
  # columns needed: state, fips code, arsenic concentration, latitude & longitude
  arsenic <- arsenic %>%
    dplyr::select(STATE,FIPS,AS_CONC,LAT_DD,LON_DD)
  # many numeric data is in character form--change from character to numeric
  arsenic$LAT_DD <- as.numeric(arsenic$LAT_DD)
  arsenic$LON_DD <- as.numeric(arsenic$LON_DD)
  arsenic$AS_CONC <- as.numeric(arsenic$AS_CONC)
  arsenic$FIPS <- as.numeric(arsenic$FIPS)
  # take out any observations with incomplete data (eg. missing fips or arsenic)
  arsenic <- arsenic[complete.cases(arsenic[,2]),]
  return(arsenic)
}

```

## Interpolation of arsenic data

The reason we need to interpolate the arsenic data is because we only have point data from the field stations that take arsenic measurements. This doesn't cover every county in the US. In order to create a continuous data of arsesnic concentrations across the US, we do a spatial interpolation that essentially takes the point arsenic concentrations and estimates the arsenic concentration across the entire area using a geospatial statistical model. In this case, the units of interpolation are 0.1 degree longitude by 0.1 degree latitude (roughly 0.25 sq mi), which means we get an arsenic concentration for every 0.25 sq mi area of the US. Then we calculate the average arsenic concentration for each county. This way we get an estimated arsenic concentration for each county.
```{r}
#' Interpolates arsenic data using invered distance weighted model
#' @param arsenic downloaded data frame including point data from field stations
#'
#' @return spatial data frame of interpolated data for  every ~0.25 sq mi of the US

Interpolate <- function(arsenic){
  us.map <- GetCountyShapefile()
  ## PREPPIND DATA FOR INTERPOLATION
  ## duplicate cleaned arsenic data to prevent overwriting
  arsenic_test <- arsenic
  ## assign coordinates--LON must be multiplied by negative to reflect that it is located in the W. Hemisphere
  ## if not, this will plot on a mirrored image of the US
  arsenic_test$x <- -1*arsenic_test$LON_DD
  arsenic_test$y <- arsenic_test$LAT_DD
  
  ## this makes arsenic_test into a spatial data
  coordinates(arsenic_test) = ~x + y
  plot(arsenic_test)
  ## define the projection for arsenic_test to match the projection for us.map
  proj4string(arsenic_test) <- proj4string(us.map)
  
  ## setting the interpolation area
  ## basically the 4 most outer points of us.map in terms of longitude and latitude
  ## you can check this using:
  summary(us.map)  ## see coordinates min & max
  x.range <- as.numeric(c(-125, -66))  # min/max longitude of the interpolation area
  y.range <- as.numeric(c(24, 50))  # min/max latitude of the interpolation area
  
  ## this creates the interpolation grid
  ## it specifies the interpolatino area, and the resolution--in this case 0.1 deg. lon. x 0.1 deg. lat.
  grd <- expand.grid(x = seq(from = x.range[1], to = x.range[2], by = 0.1), y = seq(from = y.range[1], to = y.range[2], by = 0.1))
  coordinates(grd) <- ~x + y # define spatial coordinates of grd
  proj4string(grd) <- proj4string(us.map) #with the same projections
  gridded(grd) <- TRUE
  
  ## visualization of grid and arsenic data
  plot(grd, cex = 1.5, col = "grey")
  points(arsenic_test, pch = 1, col = "red", cex = 1)
  
  ## perform the interpolation! (this will take ~20min)
  idw <- krige(AS_CONC ~ 1, arsenic_test, grd)
  
  return(idw)
}


```


```{r}
#' Summarizing the interpolated data to find average arsenic concentration per count
#' @param idw spatial data frame of interpolated data
#'
#' @return data frame of FIPSCode, arsenic values, and discrete values of arsenic

ArsenicbyCounty <- function(idw){
  library(raster)
  us.map <- GetCountyShapefile()
  
  ## convert results of interpolation from spatial pixel data to spatial raster data
  idw_raster <- raster(idw)
  ## take the mean raster value (= arsenic concentrations) within the bounds of each county defined by us.map
  avgArsenic <- extract(idw_raster, us.map, fun = mean, sp=TRUE)
  ## convert results to data frame for data manipulation
  avgArsenic_df <- as.data.frame(avgArsenic)
  
  avgArsenic_df$arsenic_discrete <- NULL
  avgArsenic_df$arsenic_discrete[avgArsenic_df$var1.pred < 3] <- "0-3"
  avgArsenic_df$arsenic_discrete[avgArsenic_df$var1.pred >= 3 & avgArsenic_df$var1.pred < 5] <- "3-5"
  avgArsenic_df$arsenic_discrete[avgArsenic_df$var1.pred >= 5 & avgArsenic_df$var1.pred < 10] <- "5-10"
  avgArsenic_df$arsenic_discrete[avgArsenic_df$var1.pred >= 10 & avgArsenic_df$var1.pred < 20] <- "10-20"
  avgArsenic_df$arsenic_discrete[avgArsenic_df$var1.pred >= 20] <- "20+"
  avgArsenic_df$arsenic_discrete <- base::as.factor(avgArsenic_df$arsenic_discrete)
  avgArsenic_df$arsenic_discrete = factor(avgArsenic_df$arsenic_discrete, levels(avgArsenic_df$arsenic_discrete) [c(1,4,5,2,3)])
  
  arsenic_df <- avgArsenic_df %>%
    dplyr::select(GEOID, var1.pred, arsenic_discrete)
  
  names(arsenic_df) <- c("FIPSCode","arsenic","arsenic_discrete")
  
  save(arsenic_df, file = "arsenic_df.RData")
  
  return(arsenic_df)
}



```

-----------------
Main Arsenic code
-----------------
```{r}
#' Downloads, interpolates, ans summarises Arsenic water quality data from USGS 
#' Source: https://water.usgs.gov/nawqa/trace/data/index.html#ARSENIC_NOV01
#' @Year:  1973 - 2001
#' @Variables: FIPS code, arsenic conentraions, latitude, longitude
#'
#' @return data frame containing FIPS code, arsenic conentraions, and discrete arsenic range

GetArsenicData <- function(){
  
  arsenic <- downloadArsenic()
  idw <- Interpolate(arsenic)
  arsenic_df <- ArsenicbyCounty(idw)
  return(arsenic_df)
}
```


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Main Code: Acquire and Join all data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

```{r}
#' Main code to aquire and join all data into the aggregate dataset including data for education, income, pollution, and forestry


#-------USER--------
# READ BELOW BEFORE RUNNING
#--------------------
#Put the first initial of your first name.
userName = "KS"

all_data=NULL

#Get reference table of counties to FIPS codes
every_county <- GetCountyReference()

#Acquire all data using support functions
edu_data <- GetEduData()
income_data <- GetIncomeData()
poll_data <- GetPollutionData()
tree_data <- GetTreeData(userName)
#arsenic_data <- GetArsenicData()


#Join all data to prepare for visualization
#all_data<-plyr::join_all(list(every_county, edu_data, income_data, poll_data, tree_data, arsenic_data), by='FIPSCode', type='full')

all_data<-plyr::join_all(list(every_county, edu_data, income_data, poll_data, tree_data), by='FIPSCode', type='full')
#View(all_data)
#all_data <- GetStateRegion4(all_data)

#all_data<-na.omit(all_data)


head(all_data)

save(all_data, file="all_data.RData")

```



